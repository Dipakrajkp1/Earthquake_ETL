{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNz8A3AGc2Iy"
      },
      "source": [
        "# **PROJECT** - EQ ETL (ð—˜ð—®ð—¿ð˜ð—µð—¾ð˜‚ð—®ð—¸ð—²ð——ð—®ð˜ð—® ð—¨ð—¦ð—šð—¦ API Data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IU_XwJFSjHsC"
      },
      "source": [
        "# **PySpark** - by *Dipakraj Patil*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgv10B42l4dM",
        "outputId": "8ed7117b-9d8c-43f8-fdff-351ca5555eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Directory '/content/drive/MyDrive/Project_EQ/Pyspark' already exists.\n"
          ]
        }
      ],
      "source": [
        "# Project - fresh start\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "project_path = r\"/content/drive/MyDrive/Project_EQ/Pyspark\"\n",
        "\n",
        "def create_directory(path):\n",
        "    # Check if the directory exists\n",
        "    if not os.path.exists(path):\n",
        "        try:\n",
        "            # Create the directory and any necessary parent directories\n",
        "            os.makedirs(path)\n",
        "            print(f\"Directory '{path}' created.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating directory '{path}': {e}\")\n",
        "    else:\n",
        "        print(f\"Directory '{path}' already exists.\")\n",
        "\n",
        "# calling a function to create a dir folder\n",
        "create_directory(project_path)\n",
        "\n",
        "# change the directory for a project\n",
        "os.chdir(project_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x44qhVj_llXH"
      },
      "source": [
        "### seismic_data_pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud9WIWNLdmat",
        "outputId": "025c32f4-24bd-494d-ab22-e6bfd55ddfd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Project_EQ/Pyspark/seismic_data_pipeline.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/MyDrive/Project_EQ/Pyspark/seismic_data_pipeline.py\"\n",
        "\n",
        "# Standard library imports\n",
        "import json\n",
        "import os\n",
        "import logging\n",
        "from decimal import Decimal, InvalidOperation\n",
        "import requests\n",
        "\n",
        "# Third-party libraries\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import from_unixtime, col, split, to_json, struct, date_format\n",
        "from pyspark.sql.types import StructType, StructField, DecimalType, LongType, StringType, IntegerType, TimestampType\n",
        "import pyarrow.parquet as pq\n",
        "import gcsfs\n",
        "\n",
        "# Google Cloud libraries\n",
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Configuration module\n",
        "import config\n",
        "\n",
        "# Set up logging configuration\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "class SeismicDataFetcher:\n",
        "    def __init__(self, source_url: str):\n",
        "        \"\"\"\n",
        "        Initialize the SeismicDataFetcher with the provided source URL.\n",
        "\n",
        "        Args:\n",
        "            source_url (str): The URL from which to fetch seismic data.\n",
        "        \"\"\"\n",
        "        self.source_url = source_url\n",
        "        self.raw_data = None  # Initialize raw_data to None\n",
        "\n",
        "    def retrieve_data(self) -> dict:\n",
        "        \"\"\"\n",
        "        Fetch seismic data from the API.\n",
        "\n",
        "        Makes a GET request to the specified source URL and retrieves the seismic data\n",
        "        in JSON format. If the request is successful, the data is stored in the\n",
        "        `raw_data` attribute.\n",
        "\n",
        "        Returns:\n",
        "            dict: The seismic data retrieved from the API.\n",
        "\n",
        "        Raises:\n",
        "            SystemExit: Exits the program if an error occurs during data retrieval.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            response = requests.get(self.source_url)\n",
        "            response.raise_for_status()  # Raise error for unsuccessful requests\n",
        "            self.raw_data = response.json()  # Store data as JSON\n",
        "            logging.info(\"Successfully retrieved seismic data.\\n\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during data retrieval: {e}\")\n",
        "            exit(1)  # Exit the program in case of an error\n",
        "\n",
        "        return self.raw_data\n",
        "\n",
        "\n",
        "class CustomJSONEncoder(json.JSONEncoder):\n",
        "    \"\"\"\n",
        "    Custom JSON encoder to handle specific data types.\n",
        "\n",
        "    This encoder extends the default JSONEncoder to provide support for\n",
        "    serializing Decimal objects as floats, ensuring compatibility with\n",
        "    JSON serialization since JSON does not natively support Decimal.\n",
        "\n",
        "    Example:\n",
        "        >>> json.dumps(Decimal('12.34'), cls=CustomJSONEncoder)\n",
        "        12.34\n",
        "    \"\"\"\n",
        "\n",
        "    def default(self, obj):\n",
        "        \"\"\"\n",
        "        Override the default method to provide custom serialization for specific types.\n",
        "\n",
        "        Args:\n",
        "            obj: The object to serialize.\n",
        "\n",
        "        Returns:\n",
        "            The serialized representation of the object.\n",
        "\n",
        "        If the object is an instance of Decimal, it converts it to a float.\n",
        "        For all other types, it falls back to the default serialization.\n",
        "        \"\"\"\n",
        "        if isinstance(obj, Decimal):\n",
        "            return float(obj)  # Convert Decimal to float\n",
        "        return super(CustomJSONEncoder, self).default(obj)\n",
        "\n",
        "\n",
        "class CloudStorageManager:\n",
        "    \"\"\"\n",
        "    A class to manage Google Cloud Storage operations for a specific bucket.\n",
        "\n",
        "    This class provides methods to upload and download JSON data, as well as\n",
        "    to read Parquet files from a specified path within a bucket.\n",
        "\n",
        "    Attributes:\n",
        "        bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "        silver_layer_path (str): The path to the silver layer in the GCS bucket.\n",
        "        gcs_client (google.cloud.storage.Client): A client to interact with Google Cloud Storage.\n",
        "        bucket (google.cloud.storage.Bucket): The specific bucket instance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, bucket_name: str, silver_layer_path: str):\n",
        "        \"\"\"\n",
        "        Initialize the CloudStorageManager with the specified bucket name and silver layer path.\n",
        "\n",
        "        Args:\n",
        "            bucket_name (str): The name of the Google Cloud Storage bucket.\n",
        "            silver_layer_path (str): The path to the silver layer in the GCS bucket.\n",
        "        \"\"\"\n",
        "        self.bucket_name = bucket_name\n",
        "        self.gcs_client = storage.Client()\n",
        "        self.bucket = self.gcs_client.bucket(bucket_name)\n",
        "        self.silver_layer_path = silver_layer_path\n",
        "\n",
        "    def upload_json_data(self, target_blob_name: str, data: dict) -> None:\n",
        "        \"\"\"\n",
        "        Upload data as JSON to Google Cloud Storage.\n",
        "\n",
        "        This method serializes the provided data using a custom JSON encoder\n",
        "        to handle Decimal serialization and uploads it to the specified blob.\n",
        "\n",
        "        Args:\n",
        "            target_blob_name (str): The name of the target blob in the bucket.\n",
        "            data (dict): The data to upload, serialized as JSON.\n",
        "        \"\"\"\n",
        "        blob = self.bucket.blob(target_blob_name)\n",
        "\n",
        "        # Use custom encoder to handle Decimal serialization\n",
        "        json_payload = json.dumps(data, cls=CustomJSONEncoder)\n",
        "        blob.upload_from_string(json_payload, content_type='application/json')\n",
        "        logging.info(f\"Data successfully uploaded to {target_blob_name} in bucket {self.bucket_name}.\\n\")\n",
        "\n",
        "    def download_json_data(self, source_blob_name: str) -> dict:\n",
        "        \"\"\"\n",
        "        Download JSON data from Google Cloud Storage and return it as a Python object.\n",
        "\n",
        "        Args:\n",
        "            source_blob_name (str): The name of the source blob in the bucket.\n",
        "\n",
        "        Returns:\n",
        "            dict: The downloaded data as a Python dictionary.\n",
        "        \"\"\"\n",
        "        blob = self.bucket.blob(source_blob_name)\n",
        "        downloaded_json = blob.download_as_text()\n",
        "        logging.info(f\"Data successfully downloaded from {source_blob_name}.\\n\")\n",
        "        return json.loads(downloaded_json)  # Convert back to Python object\n",
        "\n",
        "    def read_silver_layer_data(self) -> None:\n",
        "        \"\"\"\n",
        "        Read Parquet files from the silver layer path in the specified GCS bucket.\n",
        "\n",
        "        This method lists all Parquet files in the silver layer path, reads them\n",
        "        into an Arrow Table, converts it to a Pandas DataFrame, and returns the DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: A DataFrame containing the data from the Parquet files,\n",
        "                              or None if no files are found or an error occurs.\n",
        "        \"\"\"\n",
        "        logging.info(\"Attempting to read silver layer data...\")\n",
        "        gcs_path = f\"{self.bucket_name}/{self.silver_layer_path}\"  # Don't prefix with 'gs://'\n",
        "\n",
        "        # Initialize GCS file system using gcsfs\n",
        "        fs = gcsfs.GCSFileSystem()\n",
        "\n",
        "        # List all files in the specified directory and filter for .parquet files only\n",
        "        parquet_files = [f\"gs://{file}\" for file in fs.ls(gcs_path) if file.endswith(\".parquet\")]\n",
        "\n",
        "        # Log the filtered list of .parquet files to confirm\n",
        "        logging.info(f\"List of parquet files: {parquet_files}\")\n",
        "\n",
        "        # Check if there are any Parquet files in the list\n",
        "        if parquet_files:\n",
        "            # Try reading the .parquet files into an Arrow Table\n",
        "            try:\n",
        "                table = pq.ParquetDataset(parquet_files, filesystem=fs).read()\n",
        "                pandas_df_of_silver_layer_data = table.to_pandas()\n",
        "                logging.info(\"Pandas DataFrame created successfully from silver layer data.\\n\")\n",
        "                return pandas_df_of_silver_layer_data\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error reading Parquet files: {e}\")\n",
        "                return None\n",
        "        else:\n",
        "            logging.warning(\"No Parquet files found in the specified directory.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "class SeismicDataTransformation:\n",
        "    \"\"\"\n",
        "    A class for transforming raw seismic data into a structured format for analysis.\n",
        "\n",
        "    This class takes raw seismic data, flattens it, and converts it into a Spark DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spark_session, raw_data):\n",
        "        \"\"\"\n",
        "        Initialize the SeismicDataTransformation with a Spark session and raw data.\n",
        "\n",
        "        Args:\n",
        "            spark_session (SparkSession): The Spark session to be used for DataFrame operations.\n",
        "            raw_data (dict): The raw seismic data retrieved from the API.\n",
        "        \"\"\"\n",
        "        self.spark_session = spark_session\n",
        "        self.raw_data = raw_data\n",
        "        self.transformed_entries = None\n",
        "\n",
        "    def transform_to_flat_structure(self) -> list:\n",
        "        \"\"\"\n",
        "        Flatten and reformat the retrieved seismic data for further processing.\n",
        "\n",
        "        This method extracts relevant fields from the raw data and organizes them\n",
        "        into a flat structure suitable for analysis.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of transformed seismic data entries.\n",
        "        \"\"\"\n",
        "        events = self.raw_data['features']\n",
        "        self.transformed_entries = []\n",
        "\n",
        "        for event in events:\n",
        "            coordinates = {\n",
        "                \"longitude\": Decimal(str(event['geometry']['coordinates'][0])),\n",
        "                \"latitude\": Decimal(str(event['geometry']['coordinates'][1])),\n",
        "                \"depth\": Decimal(str(event['geometry']['coordinates'][2]))\n",
        "            }\n",
        "\n",
        "            details = event['properties']\n",
        "            details['location'] = coordinates\n",
        "\n",
        "            # Safely convert float values to Decimal\n",
        "            def safe_decimal(value):\n",
        "                try:\n",
        "                    return Decimal(str(value)) if value is not None else Decimal(\"0\")\n",
        "                except InvalidOperation:\n",
        "                    return Decimal(\"0\")\n",
        "\n",
        "            processed_entry = {\n",
        "                'magnitude': safe_decimal(details.get('mag', 0)),\n",
        "                'place': details.get('place', ''),\n",
        "                'event_time': details.get('time', 0),\n",
        "                'last_update': details.get('updated', 0),\n",
        "                'timezone_offset': int(details.get('tz', 0) or 0),\n",
        "                'info_url': details.get('url', ''),\n",
        "                'description': details.get('detail', ''),\n",
        "                'felt_reports': int(details.get('felt', 0) or 0),\n",
        "                'cdi_value': safe_decimal(details.get('cdi', 0)),\n",
        "                'mmi_value': safe_decimal(details.get('mmi', 0)),\n",
        "                'alert_status': details.get('alert', ''),\n",
        "                'event_status': details.get('status', ''),\n",
        "                'tsunami_warning': int(details.get('tsunami', 0) or 0),\n",
        "                'significance': int(details.get('sig', 0) or 0),\n",
        "                'network_code': details.get('net', ''),\n",
        "                'event_code': details.get('code', ''),\n",
        "                'event_ids': details.get('ids', ''),\n",
        "                'data_sources': details.get('sources', ''),\n",
        "                'event_types': details.get('types', ''),\n",
        "                'station_count': int(details.get('nst', 0) or 0),\n",
        "                'min_distance': safe_decimal(details.get('dmin', 0)),\n",
        "                'rms_value': safe_decimal(details.get('rms', 0)),\n",
        "                'gap_angle': safe_decimal(details.get('gap', 0)),\n",
        "                'magnitude_type': details.get('magType', ''),\n",
        "                'event_type': details.get('type', ''),\n",
        "                'location': coordinates\n",
        "            }\n",
        "\n",
        "            self.transformed_entries.append(processed_entry)\n",
        "\n",
        "        logging.info(\"Data transformed into a flat structure successfully.\\n\")\n",
        "        return self.transformed_entries\n",
        "\n",
        "    def create_spark_dataframe(self, flattened_data_in_list_form: list) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Create a Spark DataFrame from the transformed seismic data.\n",
        "\n",
        "        Args:\n",
        "            flattened_data_in_list_form (list): A list of transformed seismic data entries.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A Spark DataFrame containing the transformed data.\n",
        "        \"\"\"\n",
        "        schema = StructType([\n",
        "            StructField(\"magnitude\", DecimalType(10, 2), True),\n",
        "            StructField(\"place\", StringType(), True),\n",
        "            StructField(\"event_time\", LongType(), True),\n",
        "            StructField(\"last_update\", LongType(), True),\n",
        "            StructField(\"timezone_offset\", IntegerType(), True),\n",
        "            StructField(\"info_url\", StringType(), True),\n",
        "            StructField(\"description\", StringType(), True),\n",
        "            StructField(\"felt_reports\", IntegerType(), True),\n",
        "            StructField(\"cdi_value\", DecimalType(10, 2), True),\n",
        "            StructField(\"mmi_value\", DecimalType(10, 2), True),\n",
        "            StructField(\"alert_status\", StringType(), True),\n",
        "            StructField(\"event_status\", StringType(), True),\n",
        "            StructField(\"tsunami_warning\", IntegerType(), True),\n",
        "            StructField(\"significance\", IntegerType(), True),\n",
        "            StructField(\"network_code\", StringType(), True),\n",
        "            StructField(\"event_code\", StringType(), True),\n",
        "            StructField(\"event_ids\", StringType(), True),\n",
        "            StructField(\"data_sources\", StringType(), True),\n",
        "            StructField(\"event_types\", StringType(), True),\n",
        "            StructField(\"station_count\", IntegerType(), True),\n",
        "            StructField(\"min_distance\", DecimalType(10, 2), True),\n",
        "            StructField(\"rms_value\", DecimalType(10, 2), True),\n",
        "            StructField(\"gap_angle\", DecimalType(10, 2), True),\n",
        "            StructField(\"magnitude_type\", StringType(), True),\n",
        "            StructField(\"event_type\", StringType(), True),\n",
        "            StructField(\"location\", StructType([\n",
        "                StructField(\"longitude\", DecimalType(10, 6), True),\n",
        "                StructField(\"latitude\", DecimalType(10, 6), True),\n",
        "                StructField(\"depth\", DecimalType(10, 6), True)\n",
        "            ]), True)\n",
        "        ])\n",
        "\n",
        "        df = self.spark_session.createDataFrame(flattened_data_in_list_form, schema=schema)\n",
        "        logging.info(\"Spark DataFrame created successfully from transformed data.\\n\")\n",
        "        print('**********' * 8, 'SCHEMA > flattened df (Before Transformations)', '**********' * 8)\n",
        "        df.printSchema()\n",
        "        return df\n",
        "\n",
        "    def apply_transformations(self, flattened_df: DataFrame) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Apply transformations to the seismic data.\n",
        "\n",
        "        This method converts event_time and last_update from milliseconds to\n",
        "        timestamp format and extracts area information from the place.\n",
        "\n",
        "        Args:\n",
        "            flattened_df (DataFrame): The Spark DataFrame containing flattened seismic data.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: A transformed Spark DataFrame with applied transformations.\n",
        "        \"\"\"\n",
        "        transformed_df = flattened_df.withColumn(\"event_time\",\n",
        "                                                 from_unixtime(col(\"event_time\") / 1000).cast(TimestampType())) \\\n",
        "            .withColumn(\"last_update\", from_unixtime(col(\"last_update\") / 1000).cast(TimestampType()))\n",
        "\n",
        "        transformed_df = transformed_df.withColumn(\"area\", split(col(\"place\"), \" of \").getItem(1)).drop(\"place\")\n",
        "        logging.info(\"Transformations applied to the DataFrame successfully.\\n\")\n",
        "        print('**********' * 8, 'SCHEMA > transformed_df (After Transformations)', '**********' * 8)\n",
        "        transformed_df.printSchema()\n",
        "        return transformed_df\n",
        "\n",
        "\n",
        "class BigQueryUploader:\n",
        "    \"\"\"\n",
        "    A class to upload data from a Spark DataFrame to Google BigQuery.\n",
        "\n",
        "    This class handles the conversion of input data into a format suitable for\n",
        "    uploading to BigQuery, and provides methods to perform the upload.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, project_id: str, dataset_id: str, table_id: str, input_df, spark) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the BigQueryUploader with project and dataset information.\n",
        "\n",
        "        Args:\n",
        "            project_id (str): Google Cloud project ID.\n",
        "            dataset_id (str): BigQuery dataset ID.\n",
        "            table_id (str): BigQuery table ID.\n",
        "            input_df (DataFrame): The input DataFrame (Pandas or Spark) to be uploaded.\n",
        "        \"\"\"\n",
        "        self.client = bigquery.Client()\n",
        "        self.project_id = project_id\n",
        "        self.dataset_id = dataset_id\n",
        "        self.table_id = table_id\n",
        "        self.input_df = input_df\n",
        "        self.pyspark_df = None\n",
        "        self.spark = spark\n",
        "        logging.info(\"BigQueryUploader initialized successfully.\\n\")\n",
        "\n",
        "    def apply_transformations_for_bigquery(self) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Apply necessary transformations to the input DataFrame for BigQuery upload.\n",
        "\n",
        "        This method converts the input Pandas DataFrame into a PySpark DataFrame\n",
        "        and adds a timestamp column for record insertion.\n",
        "\n",
        "        Returns:\n",
        "            DataFrame: The transformed PySpark DataFrame ready for upload.\n",
        "        \"\"\"\n",
        "        # Convert Pandas DataFrame to PySpark DataFrame\n",
        "        self.pyspark_df = self.spark.createDataFrame(self.input_df)\n",
        "\n",
        "        # Add 'insert_dt' column\n",
        "        self.pyspark_df = self.pyspark_df \\\n",
        "            .withColumn(\"insert_dt\", F.current_timestamp()) \\\n",
        "            .withColumn(\"insert_dt\", F.col(\"insert_dt\").cast(TimestampType()))\n",
        "\n",
        "        logging.info(\"Transformations applied to the input DataFrame for BigQuery upload.\\n\")\n",
        "        return self.pyspark_df\n",
        "\n",
        "    def upload_to_bigquery(self, df) -> None:\n",
        "        \"\"\"\n",
        "        Upload the Spark DataFrame to Google BigQuery.\n",
        "\n",
        "        This method converts the Spark DataFrame to a Pandas DataFrame and uploads\n",
        "        it to BigQuery using the specified table reference. If the DataFrame is too\n",
        "        large, it can be exported to Google Cloud Storage as JSON first.\n",
        "\n",
        "        Args:\n",
        "            df (DataFrame): The Spark DataFrame to be uploaded to BigQuery.\n",
        "        \"\"\"\n",
        "        # Convert the Spark DataFrame to Pandas DataFrame\n",
        "        df_pandas = df.toPandas()\n",
        "        df_pandas['insert_dt'] = df_pandas['insert_dt'].astype('datetime64[ns]')\n",
        "        row_count = df_pandas.shape[0]\n",
        "        table_ref = f\"{self.project_id}.{self.dataset_id}.{self.table_id}\"\n",
        "\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            schema=[\n",
        "                bigquery.SchemaField(\"magnitude\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"event_time\", \"TIMESTAMP\"),\n",
        "                bigquery.SchemaField(\"last_update\", \"TIMESTAMP\"),\n",
        "                bigquery.SchemaField(\"timezone_offset\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"info_url\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"description\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"felt_reports\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"cdi_value\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"mmi_value\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"alert_status\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"event_status\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"tsunami_warning\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"significance\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"network_code\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"event_code\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"event_ids\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"data_sources\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"event_types\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"station_count\", \"INT64\"),\n",
        "                bigquery.SchemaField(\"min_distance\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"rms_value\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"gap_angle\", \"NUMERIC\"),\n",
        "                bigquery.SchemaField(\"magnitude_type\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"event_type\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"area\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"insert_dt\", \"TIMESTAMP\")\n",
        "            ],\n",
        "            write_disposition=\"WRITE_APPEND\"  # Overwrite existing data\n",
        "        )\n",
        "\n",
        "        job = self.client.load_table_from_dataframe(df_pandas, table_ref, job_config=job_config)\n",
        "        job.result()  # Wait for the job to complete\n",
        "\n",
        "        logging.info(\n",
        "            f\"<< {row_count} New Records ðŸ˜¬ >> Successfully Uploaded to BigQuery Table {self.table_id} in dataset: {self.dataset_id}\\n\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlE6Kz_8tWly"
      },
      "source": [
        "### main_hist.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lu38lXxtc8k",
        "outputId": "dc43e411-383f-4f21-ae10-ca1a6f2830e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Project_EQ/Pyspark/main_hist.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/MyDrive/Project_EQ/Pyspark/main_hist.py\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "from seismic_data_pipeline import SeismicDataFetcher, CloudStorageManager, SeismicDataTransformation, BigQueryUploader\n",
        "import config\n",
        "def main():\n",
        "    # Configure logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Create Spark session\n",
        "    spark = SparkSession.builder.master('local[*]').appName('SeismicDataPipeline').getOrCreate()\n",
        "\n",
        "    # Set Google Cloud credentials\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"default_compute_key.json\"  # Path to your service account key file\n",
        "\n",
        "    try:\n",
        "        # Fetch seismic data\n",
        "        logging.info(\"Fetching seismic data from the source API...\\n\\n\")\n",
        "        seismic_fetcher = SeismicDataFetcher(config.SOURCE_API_URL_HISTORICAL)\n",
        "        fetched_data = seismic_fetcher.retrieve_data()\n",
        "\n",
        "        # STEP 1: Ingest raw Data into GCS bucket in landing location (in .json format)\n",
        "        logging.info(\"Uploading fetched seismic data to Google Cloud Storage (landing layer)...\\n\\n\")\n",
        "        gcs_manager = CloudStorageManager(config.GCS_BUCKET, config.PYSPARK_SILVER_LAYER_PATH)\n",
        "        gcs_manager.upload_json_data(config.PYSPARK_LANDING_LOCATION, fetched_data)\n",
        "\n",
        "        # STEP 2: Download raw json data from GCS bucket for flattening and processing\n",
        "        logging.info(\"Downloading raw-JSON data from Google Cloud Storage (landing layer)...\\n\\n\")\n",
        "        data_to_process = gcs_manager.download_json_data(config.PYSPARK_LANDING_LOCATION)\n",
        "\n",
        "        # STEP 3: Perform transformations\n",
        "        logging.info(\"Transforming raw data...\\n\\n\")\n",
        "        transformations = SeismicDataTransformation(spark, data_to_process)\n",
        "        flattened_data_in_list_form = transformations.transform_to_flat_structure()  # Data in a list format\n",
        "        flattened_df = transformations.create_spark_dataframe(flattened_data_in_list_form)\n",
        "        transformed_df = transformations.apply_transformations(flattened_df)\n",
        "\n",
        "        # STEP 4: Upload transformed Data back to the GCS bucket in Silver layer location (in parquet)\n",
        "        logging.info(\"Uploading transformed data to Silver layer in Google Cloud Storage...\\n\\n\")\n",
        "        transformed_df.write.mode(\"overwrite\").parquet(config.PYSPARK_SILVER_LAYER)\n",
        "\n",
        "        # STEP 5: Read data from Silver Layer for BigQuery loading\n",
        "        logging.info(\"Reading data from Silver layer for BigQuery upload...\\n\\n\")\n",
        "        silver_layer_pandas_df = gcs_manager.read_silver_layer_data()\n",
        "\n",
        "        # TASK 6: Upload data from Silver layer to BigQuery\n",
        "        logging.info(\"Finally, Uploading data to BigQuery...\\n\\n\")\n",
        "        uploader = BigQueryUploader(config.PROJECT_ID, config.DATASET_ID, config.TABLE_ID, silver_layer_pandas_df, spark)\n",
        "        transformed_df_for_bq = uploader.apply_transformations_for_bigquery()  # Apply transformations to prepare the data for BigQuery\n",
        "        uploader.upload_to_bigquery(transformed_df_for_bq)  # Upload the transformed data to BigQuery\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during the data processing pipeline: {e}\")\n",
        "    finally:\n",
        "        # Stop Spark session\n",
        "        logging.info(\"Everything is running smoothly ðŸ˜Š \\n\\n\")\n",
        "        logging.info(\"Successfull : Now stopping Spark session...Good bye \\n\")\n",
        "        spark.stop()\n",
        "        logging.info(\"The project execution is successful! ðŸŽ‰\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1lhDbd9YAeq"
      },
      "source": [
        "## main.py - daily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCUZzhNnX_5-",
        "outputId": "ba4d18f5-3365-4dd0-b698-63e1024416c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Project_EQ/Pyspark/main_daily.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/MyDrive/Project_EQ/Pyspark/main_daily.py\"\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "from seismic_data_pipeline import SeismicDataFetcher, CloudStorageManager, SeismicDataTransformation, BigQueryUploader\n",
        "import config\n",
        "def main():\n",
        "    # Configure logging\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Create Spark session\n",
        "    spark = SparkSession.builder.master('local[*]').appName('SeismicDataPipeline').getOrCreate()\n",
        "\n",
        "    # Set Google Cloud credentials\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"default_compute_key.json\"  # Path to your service account key file\n",
        "\n",
        "    try:\n",
        "        # Fetch seismic data\n",
        "        logging.info(\"Fetching seismic data(daily) from the source API...\\n\\n\")\n",
        "        seismic_fetcher = SeismicDataFetcher(config.SOURCE_API_URL_DAILY)  # Daily data URL\n",
        "        fetched_data = seismic_fetcher.retrieve_data()\n",
        "\n",
        "        # STEP 1: Ingest raw Data into GCS bucket in landing location (in .json format)\n",
        "        logging.info(\"Uploading fetched seismic data to Google Cloud Storage (landing layer)...\\n\\n\")\n",
        "        gcs_manager = CloudStorageManager(config.GCS_BUCKET, config.PYSPARK_SILVER_LAYER_PATH)\n",
        "        gcs_manager.upload_json_data(config.PYSPARK_LANDING_LOCATION, fetched_data)\n",
        "\n",
        "        # STEP 2: Download raw json data from GCS bucket for flattening and processing\n",
        "        logging.info(\"Downloading raw JSON data from Google Cloud Storage...\\n\\n\")\n",
        "        data_to_process = gcs_manager.download_json_data(config.PYSPARK_LANDING_LOCATION)\n",
        "\n",
        "        # STEP 3: Perform transformations\n",
        "        logging.info(\"Transforming raw data...\\n\\n\")\n",
        "        transformations = SeismicDataTransformation(spark, data_to_process)\n",
        "        flattened_data_in_list_form = transformations.transform_to_flat_structure()  # Data in a list format\n",
        "        flattened_df = transformations.create_spark_dataframe(flattened_data_in_list_form)\n",
        "        transformed_df = transformations.apply_transformations(flattened_df)\n",
        "\n",
        "        # STEP 4: Upload transformed Data back to the GCS bucket in Silver layer location (in parquet)\n",
        "        logging.info(\"Uploading transformed data to Silver layer in Google Cloud Storage...\\n\\n\")\n",
        "        transformed_df.write.mode(\"overwrite\").parquet(config.PYSPARK_SILVER_LAYER)\n",
        "\n",
        "        # STEP 5: Read data from Silver Layer for BigQuery loading\n",
        "        logging.info(\"Reading data from Silver layer for BigQuery upload...\\n\\n\")\n",
        "        silver_layer_pandas_df = gcs_manager.read_silver_layer_data()\n",
        "\n",
        "        # TASK 6: Upload data from Silver layer to BigQuery\n",
        "        logging.info(\"Uploading (daily) data to BigQuery...\")\n",
        "        uploader = BigQueryUploader(config.PROJECT_ID, config.DATASET_ID, config.TABLE_ID, silver_layer_pandas_df, spark)\n",
        "        transformed_df_for_bq = uploader.apply_transformations_for_bigquery()  # Apply transformations to prepare the data for BigQuery\n",
        "        uploader.upload_to_bigquery(transformed_df_for_bq)  # Upload the transformed data to BigQuery\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred during the data processing pipeline: {e}\")\n",
        "    finally:\n",
        "        # Stop Spark session\n",
        "        logging.info(\"Everything is running smoothly ðŸ˜Š \\n\\n\")\n",
        "        logging.info(\"Successfull : Now stopping Spark session...Good bye \\n\")\n",
        "        spark.stop()\n",
        "        logging.info(\"The project execution is successful! ðŸŽ‰\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8735ZL6n88J"
      },
      "source": [
        "## default_compute_key.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"/content/drive/MyDrive/Project_EQ/Pyspark/default_compute_key.json\"\n",
        "\n",
        "{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"project-dipakraj1\",\n",
        "  \"private_key_id\": \"49fe9764106942760488ba457e6f3880edee41d5\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC69MKUe7po5Jsc\\nminDO4aZWWbkKkqFHREMkCuov/3CDJuvrUiXrDN85Ev3VVgp664mFOxYxrRfaG8f\\nzymHJImeFJh/SXZ3RO9TK04LEjwQszAD2LpT9EaZIHeQChjTkdvDMroasD2bHSBF\\nvt2ftLPYEpRVevmHATW3qDCGGht0/inqi+rsxw1p3XHXyqiRQdLA7IWH9g7lF43o\\nHnrCEd6NPkeT/9OYpQHBYGo30OJ5KtvdFGhQJk/Iom0EG8Z4wISHL7//+/JPckXo\\n1tS19zM5gfWV+qZCvwwasjbI8novAdgIVnvP8GB0V9Umm0lhRex6v4+fR9o7s677\\nlVqCR5uJAgMBAAECggEAESfwyytnecSwrzk+Z9oIif34QSvYVsrKC7sXJo8nvCzo\\nBIEETlbRCr7wcav1QTONcZUt8x+Y4cB1oheimfIyZr1BGT0mj99Vju5EZSOxO18W\\nIuQ6hQW/jMG79w3WJWQXJJx9E4HkxV0w3vWa5NLyRfu+fPGiAE+OHGJkwYmXx257\\nKyftMqEwRPfZyiut3JT5HZxuHuXzlHPAOEs1mAGWeZFXep8rjp5ViUdWLLDSWhjG\\nD0hFCLCRKPzot6zUj7K+HMoJGXPk0iKta3mXSvGKD3hWM1nWYExzGZ3Bf8DoS80O\\nn3FCiOU/mGqdnrPwjOuh97tPbbvXt5+lCo4nwgOuYQKBgQDrmhtjEuBWHuz9MjUK\\nB/arnSOexowklqoOLpgzmrD8acuXChfQPEDy/AMuIiDM1VpNwkllJRawgC5/f1uk\\n3EqPGQYTcQ3DodRErw4Za7Udr1BOdSq4jFQMvfhlbu8twhSceSNGWwT04XJUB/l0\\nbhjZAXDh1YHOrNevfR2EGpTrpQKBgQDLJHalMu+IHxb9SX3HuvoIS9FBiaW+nXYR\\npMalAXF20c/ISvbRprfS2kUqNIyfxj8NolA8FRz+px6iqQmB4EHzEQTmjowuQFNc\\nUDd1isE7jVO4gkoxjyK+ZtIIn/nufeVh2UWvG4mUXhqgem0UgkeZrFVws8E3QT5i\\nqtuEU/h7FQKBgGAdTm7slHiNxUDF6r1c4lTAlnd/qEyE6ns+pvjmcq2Gu3eytmRN\\nMHHwPZvkcF0f7OcsC6UnrEn1AQMigdCPzhryqnfj9ymIK4CZqbbVKd4iVzCqu/Lt\\nzJwmI9+9kfWo55+uC6X4G92K24lMd4f4IkrlO5fl8j7OZG9DK3dpILfBAoGBAK8L\\nRJElSvHomE65PQEWRM1RR4dJrOCLnmmlquYUUEkqkfjtX7FYD55JJsAvd91VXo/J\\nBYpARzPHncZcEi0vqKRiTe69dl2EYFBzbtaCMe+24CVlX7lEtZOL1gBImXzATx/5\\nsOoquGp4jEksNUIu4LOypAzxV9TdC3zJ7U4hbUPhAoGAIx6XFi08xsDS6mf8/pC5\\nkkXl4y4+VZcHWHtGOdM7Qwg83FwujbTWVLfVB0UrVcaspco6csiYdPCFRenO0gBc\\n/XU0nsuQcXScU1OlPEvFe5rK+TZt+lcSiK85h5oqZ/u2NCcTIIFvAqB+GHwS9vr2\\nx5sxhfjDtrW5Op3Gn9Q3wAY=\\n-----END PRIVATE KEY-----\\n\",\n",
        "  \"client_email\": \"771574848243-compute@developer.gserviceaccount.com\",\n",
        "  \"client_id\": \"115938029565310472294\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/771574848243-compute%40developer.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iR6WRk90HWH",
        "outputId": "3fe6a83d-faf5-4241-98fb-ec689d2dd5f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Project_EQ/Pyspark/default_compute_key.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3EfeBJQ0ETZ"
      },
      "source": [
        "## config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmUkwDkBnklT",
        "outputId": "c28b0cb9-c94c-4b82-b442-7c63e71de2a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/drive/MyDrive/Project_EQ/Pyspark/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile \"/content/drive/MyDrive/Project_EQ/Pyspark/config.py\"\n",
        "\n",
        "from datetime import datetime\n",
        "date = datetime.now().strftime(\"%y%m%d\")\n",
        "# import os\n",
        "# SERVICE_ACCOUNT_KEY = os.getenv('SERVICE_ACCOUNT_KEY', r\"/tmp/default_compute_key.json\")\n",
        "# SERVICE_ACCOUNT_KEY = r\"/content/drive/MyDrive/Colab Notebooks/Project_Earthquake/default_compute_key.json\"\n",
        "\n",
        "PROJECT_ID = \"project-dipakraj1\"\n",
        "DATASET_ID = \"earthquake_db\"\n",
        "TABLE_ID = \"EQ_table_using_pyspark_colab\"\n",
        "GCS_BUCKET = \"production-bucket-dipakraj1\"\n",
        "LOCATION = \"us-central1\"\n",
        "LOCATION_FOR_DATASET = \"US\"\n",
        "\n",
        "SOURCE_API_URL_HISTORICAL = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_month.geojson\"\n",
        "SOURCE_API_URL_DAILY = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson\"\n",
        "\n",
        "PYSPARK_LANDING_LOCATION = f\"pyspark_bronz/landing/{date}/raw_data_pyspark_{date}.json\"\n",
        "\n",
        "PYSPARK_SILVER_LAYER = f\"gs://{GCS_BUCKET}/pyspark_silver/{date}/transformed_data_Pyspark_{date}/\"\n",
        "PYSPARK_SILVER_LAYER_PATH = f\"pyspark_silver/{date}/transformed_data_Pyspark_{date}/\"\n",
        "TEMP_LOCATION = f\"gs://{GCS_BUCKET}/temp\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !if bq show --project_id=project-dipakraj earthquake_db.audit_table > /dev/null 2>&1; then echo \"Table exists.\" else echo \"Table does not exist.\" fi"
      ],
      "metadata": {
        "id": "M-1bnXhq3G_9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !bq show --project_id=project-dipakraj earthquake_db.audit_table && echo \"Table exists.\" || echo \"Table does not exist.\""
      ],
      "metadata": {
        "id": "LLYJqfXs4MiQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFU7zqLue3Ah"
      },
      "source": [
        "## **to execute** *or* **to run**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating required things\n",
        "import config\n",
        "project_id = config.PROJECT_ID\n",
        "bucket_name = config.GCS_BUCKET #\"earthquake_data_bucket\"\n",
        "dataset = config.DATASET_ID #\"Test_earthquake_db\"\n",
        "location = config.LOCATION_FOR_DATASET\n",
        "\n",
        "# Authentication\n",
        "!gcloud config unset project\n",
        "!gcloud auth login\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "# create GCS storage bucket\n",
        "!gcloud storage buckets create gs://{bucket_name} --location={location}\n",
        "!gcloud services enable bigquery.googleapis.com\n",
        "\n",
        "# # create bigquery dataset\n",
        "!bq mk --dataset --description \"This dataset is for earthquake data loading (historical plus incremental)\" --location {location} {project_id}:{dataset}\n",
        "\n",
        "# # Create a dataproc-cluster with basic configurations: wrt free trial account GCP\n",
        "!gcloud dataproc clusters create cluster-for-eq-project --region us-central1 --zone us-central1-f --master-machine-type e2-standard-2 --master-boot-disk-type pd-balanced --master-boot-disk-size 30 --num-workers 2 --worker-machine-type e2-standard-2 --worker-boot-disk-type pd-balanced --worker-boot-disk-size 30 --image-version 2.0-debian10 --max-idle 7200s --project {project_id}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WGG7CZvRySWn",
        "outputId": "3b4ccab5-570a-4909-e033-6bad223d813e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unset property [core/project].\n",
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=BH0gzov8YY0gdLzbEtosxLlTksYSpU&prompt=consent&token_usage=remote&access_type=offline&code_challenge=swojc5q2_n5aFSqyPRtO5RSk8wb1mhe5dklyqHn62nA&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVG7fiTZ5Vg9gDijEl2lecQ3EnpJK2UDgy-kGYSeegsQRkSvCzxGVX21xi1lJUOdP_j-jQ\n",
            "\n",
            "You are now logged in as [dipakrajpatil.gcp1@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "Updated property [core/project].\n",
            "Creating gs://production-bucket-dipakraj1/...\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.storage.buckets.create) HTTPError 409: The requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again.\n",
            "BigQuery error in mk operation: Dataset 'project-dipakraj1:earthquake_db' already exists.\n",
            "Waiting on operation [projects/project-dipakraj1/regions/us-central1/operations/bdaf49e3-51dd-38bb-8a51-7031b602d3d8].\n",
            "\n",
            "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
            "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '771574848243-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '771574848243' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=771574848243'.\n",
            "\u001b[1;33mWARNING:\u001b[0m The firewall rules for specified network or subnetwork would allow ingress traffic from 0.0.0.0/0, which could be a security risk.\n",
            "\u001b[1;33mWARNING:\u001b[0m The specified custom staging bucket 'dataproc-staging-us-central1-771574848243-4hclxhic' is not using uniform bucket level access IAM configuration. It is recommended to update bucket to enable the same. See https://cloud.google.com/storage/docs/uniform-bucket-level-access.\n",
            "Created [https://dataproc.googleapis.com/v1/projects/project-dipakraj1/regions/us-central1/clusters/cluster-for-eq-project] Cluster placed in zone [us-central1-f].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26WtbVKGxJCk"
      },
      "source": [
        "### Run a job -local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DaO8wnv1dPI",
        "outputId": "feb4fa78-dbed-46c8-c3d5-df93f3c4dde7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files:\n",
            " ['__pycache__', 'seismic_data_pipeline.py', 'main_hist.py', 'default_compute_key.json', 'main_daily.py', 'config.py', 'my_spark_job_historical.zip'] \n",
            "\n",
            "updating: seismic_data_pipeline.py (deflated 75%)\n",
            "updating: main_hist.py (deflated 62%)\n",
            "updating: main_daily.py (deflated 62%)\n",
            "updating: default_compute_key.json (deflated 30%)\n",
            "updating: config.py (deflated 51%)\n",
            "updating: __pycache__/ (stored 0%)\n",
            "updating: __pycache__/config.cpython-310.pyc (deflated 35%)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Project_EQ/Pyspark\")\n",
        "print('Files:\\n', os.listdir(),'\\n')\n",
        "\n",
        "# zipping all files to submit DataProc job\n",
        "!zip -r my_spark_job_historical.zip ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "MQvKyvqfysHD"
      },
      "outputs": [],
      "source": [
        "# local testing\n",
        "\n",
        "#uncomment if want to test locally\n",
        "# !spark-submit --py-files my_spark_job_historical.zip main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrCipR6KeaeQ"
      },
      "source": [
        "### Run DataProc job - Historical Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execute main_hist.py"
      ],
      "metadata": {
        "id": "sTgEQ-oOt_lt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxUwgKBc4HQG",
        "outputId": "1e6c17e0-43ba-461d-aca8-d8e9ebc35f4f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [8ceb24eb0adb43039accb30aeeb6e730] submitted.\n",
            "Waiting for job output...\n",
            "24/11/05 03:28:35 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "24/11/05 03:28:35 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "24/11/05 03:28:35 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/11/05 03:28:35 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "24/11/05 03:28:35 INFO org.sparkproject.jetty.util.log: Logging initialized @11435ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "24/11/05 03:28:35 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_422-b05\n",
            "24/11/05 03:28:35 INFO org.sparkproject.jetty.server.Server: Started @11656ms\n",
            "24/11/05 03:28:35 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@13682bd8{HTTP/1.1, (http/1.1)}{0.0.0.0:44917}\n",
            "24/11/05 03:28:36 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics\n",
            "24/11/05 03:28:36 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 executor metrics\n",
            "24/11/05 03:28:37 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "2024-11-05 03:28:40,842 - INFO - Fetching seismic data from the source API...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:41,250 - INFO - Successfully retrieved seismic data.\n",
            "\n",
            "2024-11-05 03:28:41,251 - INFO - Uploading fetched seismic data to Google Cloud Storage (landing layer)...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:41,980 - INFO - Data successfully uploaded to pyspark_bronz/landing/241105/raw_data_pyspark_241105.json in bucket production-bucket-dipakraj1.\n",
            "\n",
            "2024-11-05 03:28:41,980 - INFO - Downloading raw-JSON data from Google Cloud Storage (landing layer)...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:42,153 - INFO - Data successfully downloaded from pyspark_bronz/landing/241105/raw_data_pyspark_241105.json.\n",
            "\n",
            "2024-11-05 03:28:42,287 - INFO - Transforming raw data...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:42,469 - INFO - Data transformed into a flat structure successfully.\n",
            "\n",
            "2024-11-05 03:28:46,084 - INFO - Spark DataFrame created successfully from transformed data.\n",
            "\n",
            "******************************************************************************** SCHEMA > flattened df (Before Transformations) ********************************************************************************\n",
            "root\n",
            " |-- magnitude: decimal(10,2) (nullable = true)\n",
            " |-- place: string (nullable = true)\n",
            " |-- event_time: long (nullable = true)\n",
            " |-- last_update: long (nullable = true)\n",
            " |-- timezone_offset: integer (nullable = true)\n",
            " |-- info_url: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- felt_reports: integer (nullable = true)\n",
            " |-- cdi_value: decimal(10,2) (nullable = true)\n",
            " |-- mmi_value: decimal(10,2) (nullable = true)\n",
            " |-- alert_status: string (nullable = true)\n",
            " |-- event_status: string (nullable = true)\n",
            " |-- tsunami_warning: integer (nullable = true)\n",
            " |-- significance: integer (nullable = true)\n",
            " |-- network_code: string (nullable = true)\n",
            " |-- event_code: string (nullable = true)\n",
            " |-- event_ids: string (nullable = true)\n",
            " |-- data_sources: string (nullable = true)\n",
            " |-- event_types: string (nullable = true)\n",
            " |-- station_count: integer (nullable = true)\n",
            " |-- min_distance: decimal(10,2) (nullable = true)\n",
            " |-- rms_value: decimal(10,2) (nullable = true)\n",
            " |-- gap_angle: decimal(10,2) (nullable = true)\n",
            " |-- magnitude_type: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- location: struct (nullable = true)\n",
            " |    |-- longitude: decimal(10,6) (nullable = true)\n",
            " |    |-- latitude: decimal(10,6) (nullable = true)\n",
            " |    |-- depth: decimal(10,6) (nullable = true)\n",
            "\n",
            "2024-11-05 03:28:47,725 - INFO - Transformations applied to the DataFrame successfully.\n",
            "\n",
            "******************************************************************************** SCHEMA > transformed_df (After Transformations) ********************************************************************************\n",
            "root\n",
            " |-- magnitude: decimal(10,2) (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- last_update: timestamp (nullable = true)\n",
            " |-- timezone_offset: integer (nullable = true)\n",
            " |-- info_url: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- felt_reports: integer (nullable = true)\n",
            " |-- cdi_value: decimal(10,2) (nullable = true)\n",
            " |-- mmi_value: decimal(10,2) (nullable = true)\n",
            " |-- alert_status: string (nullable = true)\n",
            " |-- event_status: string (nullable = true)\n",
            " |-- tsunami_warning: integer (nullable = true)\n",
            " |-- significance: integer (nullable = true)\n",
            " |-- network_code: string (nullable = true)\n",
            " |-- event_code: string (nullable = true)\n",
            " |-- event_ids: string (nullable = true)\n",
            " |-- data_sources: string (nullable = true)\n",
            " |-- event_types: string (nullable = true)\n",
            " |-- station_count: integer (nullable = true)\n",
            " |-- min_distance: decimal(10,2) (nullable = true)\n",
            " |-- rms_value: decimal(10,2) (nullable = true)\n",
            " |-- gap_angle: decimal(10,2) (nullable = true)\n",
            " |-- magnitude_type: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- location: struct (nullable = true)\n",
            " |    |-- longitude: decimal(10,6) (nullable = true)\n",
            " |    |-- latitude: decimal(10,6) (nullable = true)\n",
            " |    |-- depth: decimal(10,6) (nullable = true)\n",
            " |-- area: string (nullable = true)\n",
            "\n",
            "2024-11-05 03:28:47,729 - INFO - Uploading transformed data to Silver layer in Google Cloud Storage...\n",
            "\n",
            "\n",
            "24/11/05 03:28:49 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "24/11/05 03:28:50 WARN org.apache.spark.scheduler.TaskSetManager: Stage 0 contains a task of very large size (1739 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000\n",
            "24/11/05 03:28:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on\n",
            "24/11/05 03:28:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\n",
            "24/11/05 03:28:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/attempt_202411050328504062815254312071739_0000_m_000000_0/' directory.\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/attempt_202411050328503294061192039616608_0000_m_000001_1/' directory.\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/' directory.\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/' directory.\n",
            "24/11/05 03:28:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/' directory.\n",
            "2024-11-05 03:28:58,255 - INFO - Reading data from Silver layer for BigQuery upload...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:58,255 - INFO - Attempting to read silver layer data...\n",
            "2024-11-05 03:28:58,408 - INFO - List of parquet files: ['gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/part-00000-6c7302fc-70f2-4f39-bb94-64116d7a1b7e-c000.snappy.parquet', 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/part-00001-6c7302fc-70f2-4f39-bb94-64116d7a1b7e-c000.snappy.parquet']\n",
            "2024-11-05 03:28:59,155 - INFO - Pandas DataFrame created successfully from silver layer data.\n",
            "\n",
            "2024-11-05 03:28:59,156 - INFO - Finally, Uploading data to BigQuery...\n",
            "\n",
            "\n",
            "2024-11-05 03:28:59,172 - INFO - BigQueryUploader initialized successfully.\n",
            "\n",
            "2024-11-05 03:29:01,722 - INFO - Transformations applied to the input DataFrame for BigQuery upload.\n",
            "\n",
            "24/11/05 03:29:02 WARN org.apache.spark.scheduler.TaskSetManager: Stage 1 contains a task of very large size (1494 KiB). The maximum recommended task size is 1000 KiB.\n",
            "/opt/conda/default/lib/python3.8/site-packages/google/cloud/bigquery/_pandas_helpers.py:377: UserWarning: Pyarrow could not determine the type of columns: location.\n",
            "  warnings.warn(\n",
            "2024-11-05 03:29:10,077 - INFO - << 8301 New Records ðŸ˜¬ >> Successfully Uploaded to BigQuery Table EQ_table_using_pyspark_colab in dataset: earthquake_db\n",
            "\n",
            "2024-11-05 03:29:10,077 - INFO - Everything is running smoothly ðŸ˜Š \n",
            "\n",
            "\n",
            "2024-11-05 03:29:10,077 - INFO - Successfull : Now stopping Spark session...Good bye \n",
            "\n",
            "24/11/05 03:29:10 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@13682bd8{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "24/11/05 03:29:10 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=4, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=2, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=89, gcs_api_total_request_count=116, op_create=4, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=6650, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=2, op_get_file_status=14, stream_read_total_bytes=0, op_glob_status=2, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=805806, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=4, stream_read_bytes=0, gcs_api_client_non_found_response_count=53, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=2, op_xattr_get_named_map=0, gcs_api_client_side_error_count=140, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=3, op_list_files=0, files_deleted=0, op_mkdirs=2, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=2, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=85, op_delete_min=41, op_mkdirs_min=172, op_create_non_recursive_min=0, op_glob_status_min=47, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=29, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=26, op_open_min=0, op_rename_min=291, delegation_tokens_issued_min=0, stream_write_close_operations_min=111, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=444, op_get_file_status_max=387, stream_write_close_operations_max=138, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=336, op_rename_max=298, op_create_max=279, op_delete_max=198, op_list_status_max=45, op_xattr_get_named_map_max=0, stream_write_operations_max=3, op_hsync_max=0, op_list_status_mean=37, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=254, stream_write_close_operations_mean=121, op_rename_mean=294, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=181, op_glob_status_mean=245, op_delete_mean=142, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=68, stream_write_operations_duration=0, stream_read_operations_duration=0]\n",
            "24/11/05 03:29:10 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down executor plugin. metrics=[files_created=4, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=2, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=89, gcs_api_total_request_count=117, op_create=4, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=6650, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=2, op_get_file_status=14, stream_read_total_bytes=0, op_glob_status=2, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=805806, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=4, stream_read_bytes=0, gcs_api_client_non_found_response_count=53, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=2, op_xattr_get_named_map=0, gcs_api_client_side_error_count=140, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=4, op_list_files=0, files_deleted=0, op_mkdirs=2, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=2, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=85, op_delete_min=41, op_mkdirs_min=172, op_create_non_recursive_min=0, op_glob_status_min=47, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=29, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=26, op_open_min=0, op_rename_min=291, delegation_tokens_issued_min=0, stream_write_close_operations_min=111, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=444, op_get_file_status_max=387, stream_write_close_operations_max=138, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=336, op_rename_max=298, op_create_max=279, op_delete_max=198, op_list_status_max=45, op_xattr_get_named_map_max=0, stream_write_operations_max=3, op_hsync_max=0, op_list_status_mean=37, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=254, stream_write_close_operations_mean=121, op_rename_mean=294, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=181, op_glob_status_mean=245, op_delete_mean=142, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=68, stream_write_operations_duration=0, stream_read_operations_duration=0]\n",
            "2024-11-05 03:29:10,864 - INFO - The project execution is successful! ðŸŽ‰\n",
            "Job [8ceb24eb0adb43039accb30aeeb6e730] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/driveroutput\n",
            "jobUuid: e1bedeee-9e6e-3be8-96f2-8e74c8a2bf70\n",
            "placement:\n",
            "  clusterName: cluster-for-eq-project\n",
            "  clusterUuid: 7add3a5b-0925-4969-9de0-3d7a3e8b6b8a\n",
            "pysparkJob:\n",
            "  fileUris:\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/staging/my_spark_job_historical.zip\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/staging/default_compute_key.json\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/staging/main_hist.py\n",
            "  pythonFileUris:\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/8ceb24eb0adb43039accb30aeeb6e730/staging/my_spark_job_historical.zip\n",
            "reference:\n",
            "  jobId: 8ceb24eb0adb43039accb30aeeb6e730\n",
            "  projectId: project-dipakraj1\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2024-11-05T03:29:11.565754Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2024-11-05T03:28:21.208964Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2024-11-05T03:28:21.242870Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2024-11-05T03:28:21.653988Z'\n"
          ]
        }
      ],
      "source": [
        "# run main_hist.py\n",
        "\n",
        "!gcloud dataproc jobs submit pyspark \\\n",
        "    --cluster cluster-for-eq-project \\\n",
        "    --region us-central1 \\\n",
        "    --files my_spark_job_historical.zip,default_compute_key.json \\\n",
        "    --py-files my_spark_job_historical.zip \\\n",
        "    main_hist.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execute main_daily.py"
      ],
      "metadata": {
        "id": "W_LIxuh5t2A9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZycyyEL4indO",
        "outputId": "f8577fad-15b2-48fa-e8aa-08d5ba791ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Job [f0b9ce7f9ed14764b564bbc847cd1d93] submitted.\n",
            "Waiting for job output...\n",
            "24/11/05 03:29:25 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "24/11/05 03:29:25 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "24/11/05 03:29:25 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/11/05 03:29:25 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "24/11/05 03:29:25 INFO org.sparkproject.jetty.util.log: Logging initialized @8670ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "24/11/05 03:29:25 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_422-b05\n",
            "24/11/05 03:29:26 INFO org.sparkproject.jetty.server.Server: Started @8907ms\n",
            "24/11/05 03:29:26 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@60f6a977{HTTP/1.1, (http/1.1)}{0.0.0.0:44493}\n",
            "24/11/05 03:29:26 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 driver metrics\n",
            "24/11/05 03:29:26 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Registered 128 executor metrics\n",
            "24/11/05 03:29:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "2024-11-05 03:29:30,525 - INFO - Fetching seismic data(daily) from the source API...\n",
            "\n",
            "\n",
            "2024-11-05 03:29:30,633 - INFO - Successfully retrieved seismic data.\n",
            "\n",
            "2024-11-05 03:29:30,634 - INFO - Uploading fetched seismic data to Google Cloud Storage (landing layer)...\n",
            "\n",
            "\n",
            "2024-11-05 03:29:30,792 - INFO - Data successfully uploaded to pyspark_bronz/landing/241105/raw_data_pyspark_241105.json in bucket production-bucket-dipakraj1.\n",
            "\n",
            "2024-11-05 03:29:30,792 - INFO - Downloading raw JSON data from Google Cloud Storage...\n",
            "\n",
            "\n",
            "2024-11-05 03:29:30,858 - INFO - Data successfully downloaded from pyspark_bronz/landing/241105/raw_data_pyspark_241105.json.\n",
            "\n",
            "2024-11-05 03:29:30,860 - INFO - Transforming raw data...\n",
            "\n",
            "\n",
            "2024-11-05 03:29:30,862 - INFO - Data transformed into a flat structure successfully.\n",
            "\n",
            "2024-11-05 03:29:34,018 - INFO - Spark DataFrame created successfully from transformed data.\n",
            "\n",
            "******************************************************************************** SCHEMA > flattened df (Before Transformations) ********************************************************************************\n",
            "root\n",
            " |-- magnitude: decimal(10,2) (nullable = true)\n",
            " |-- place: string (nullable = true)\n",
            " |-- event_time: long (nullable = true)\n",
            " |-- last_update: long (nullable = true)\n",
            " |-- timezone_offset: integer (nullable = true)\n",
            " |-- info_url: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- felt_reports: integer (nullable = true)\n",
            " |-- cdi_value: decimal(10,2) (nullable = true)\n",
            " |-- mmi_value: decimal(10,2) (nullable = true)\n",
            " |-- alert_status: string (nullable = true)\n",
            " |-- event_status: string (nullable = true)\n",
            " |-- tsunami_warning: integer (nullable = true)\n",
            " |-- significance: integer (nullable = true)\n",
            " |-- network_code: string (nullable = true)\n",
            " |-- event_code: string (nullable = true)\n",
            " |-- event_ids: string (nullable = true)\n",
            " |-- data_sources: string (nullable = true)\n",
            " |-- event_types: string (nullable = true)\n",
            " |-- station_count: integer (nullable = true)\n",
            " |-- min_distance: decimal(10,2) (nullable = true)\n",
            " |-- rms_value: decimal(10,2) (nullable = true)\n",
            " |-- gap_angle: decimal(10,2) (nullable = true)\n",
            " |-- magnitude_type: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- location: struct (nullable = true)\n",
            " |    |-- longitude: decimal(10,6) (nullable = true)\n",
            " |    |-- latitude: decimal(10,6) (nullable = true)\n",
            " |    |-- depth: decimal(10,6) (nullable = true)\n",
            "\n",
            "2024-11-05 03:29:34,871 - INFO - Transformations applied to the DataFrame successfully.\n",
            "\n",
            "******************************************************************************** SCHEMA > transformed_df (After Transformations) ********************************************************************************\n",
            "root\n",
            " |-- magnitude: decimal(10,2) (nullable = true)\n",
            " |-- event_time: timestamp (nullable = true)\n",
            " |-- last_update: timestamp (nullable = true)\n",
            " |-- timezone_offset: integer (nullable = true)\n",
            " |-- info_url: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- felt_reports: integer (nullable = true)\n",
            " |-- cdi_value: decimal(10,2) (nullable = true)\n",
            " |-- mmi_value: decimal(10,2) (nullable = true)\n",
            " |-- alert_status: string (nullable = true)\n",
            " |-- event_status: string (nullable = true)\n",
            " |-- tsunami_warning: integer (nullable = true)\n",
            " |-- significance: integer (nullable = true)\n",
            " |-- network_code: string (nullable = true)\n",
            " |-- event_code: string (nullable = true)\n",
            " |-- event_ids: string (nullable = true)\n",
            " |-- data_sources: string (nullable = true)\n",
            " |-- event_types: string (nullable = true)\n",
            " |-- station_count: integer (nullable = true)\n",
            " |-- min_distance: decimal(10,2) (nullable = true)\n",
            " |-- rms_value: decimal(10,2) (nullable = true)\n",
            " |-- gap_angle: decimal(10,2) (nullable = true)\n",
            " |-- magnitude_type: string (nullable = true)\n",
            " |-- event_type: string (nullable = true)\n",
            " |-- location: struct (nullable = true)\n",
            " |    |-- longitude: decimal(10,6) (nullable = true)\n",
            " |    |-- latitude: decimal(10,6) (nullable = true)\n",
            " |    |-- depth: decimal(10,6) (nullable = true)\n",
            " |-- area: string (nullable = true)\n",
            "\n",
            "2024-11-05 03:29:34,876 - INFO - Uploading transformed data to Silver layer in Google Cloud Storage...\n",
            "\n",
            "\n",
            "24/11/05 03:29:36 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
            "24/11/05 03:29:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/' directory.\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000\n",
            "24/11/05 03:29:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on\n",
            "24/11/05 03:29:40 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\n",
            "24/11/05 03:29:40 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]\n",
            "24/11/05 03:29:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/attempt_202411050329378336243418856308386_0000_m_000001_1/' directory.\n",
            "24/11/05 03:29:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/attempt_202411050329371086056706084641595_0000_m_000000_0/' directory.\n",
            "24/11/05 03:29:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/' directory.\n",
            "24/11/05 03:29:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "24/11/05 03:29:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/_temporary/0/_temporary/' directory.\n",
            "24/11/05 03:29:43 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/' directory.\n",
            "2024-11-05 03:29:43,493 - INFO - Reading data from Silver layer for BigQuery upload...\n",
            "\n",
            "\n",
            "2024-11-05 03:29:43,493 - INFO - Attempting to read silver layer data...\n",
            "2024-11-05 03:29:43,643 - INFO - List of parquet files: ['gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/part-00000-72caff32-90f7-448f-be78-efbb9e8c8fd9-c000.snappy.parquet', 'gs://production-bucket-dipakraj1/pyspark_silver/241105/transformed_data_Pyspark_241105/part-00001-72caff32-90f7-448f-be78-efbb9e8c8fd9-c000.snappy.parquet']\n",
            "2024-11-05 03:29:43,891 - INFO - Pandas DataFrame created successfully from silver layer data.\n",
            "\n",
            "2024-11-05 03:29:43,891 - INFO - Uploading (daily) data to BigQuery...\n",
            "2024-11-05 03:29:43,902 - INFO - BigQueryUploader initialized successfully.\n",
            "\n",
            "2024-11-05 03:29:43,969 - ERROR - An error occurred during the data processing pipeline: Some of types cannot be determined after inferring\n",
            "2024-11-05 03:29:43,969 - INFO - Everything is running smoothly ðŸ˜Š \n",
            "\n",
            "\n",
            "2024-11-05 03:29:43,969 - INFO - Successfull : Now stopping Spark session...Good bye \n",
            "\n",
            "24/11/05 03:29:44 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@60f6a977{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "24/11/05 03:29:44 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down driver plugin. metrics=[files_created=4, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=2, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=91, gcs_api_total_request_count=127, op_create=4, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=6552, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=2, op_get_file_status=15, stream_read_total_bytes=0, op_glob_status=2, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=156334, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=5, stream_read_bytes=0, gcs_api_client_non_found_response_count=56, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=2, op_xattr_get_named_map=0, gcs_api_client_side_error_count=146, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=3, op_list_files=0, files_deleted=0, op_mkdirs=2, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=2, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=84, op_delete_min=43, op_mkdirs_min=147, op_create_non_recursive_min=0, op_glob_status_min=55, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=26, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=23, op_open_min=0, op_rename_min=269, delegation_tokens_issued_min=0, stream_write_close_operations_min=90, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=378, op_get_file_status_max=332, stream_write_close_operations_max=171, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=262, op_rename_max=289, op_create_max=147, op_delete_max=257, op_list_status_max=28, op_xattr_get_named_map_max=0, stream_write_operations_max=3, op_hsync_max=0, op_list_status_mean=27, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=204, stream_write_close_operations_mean=125, op_rename_mean=279, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=119, op_glob_status_mean=216, op_delete_mean=165, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=57, stream_write_operations_duration=0, stream_read_operations_duration=0]\n",
            "24/11/05 03:29:44 INFO com.google.cloud.dataproc.DataprocSparkPlugin: Shutting down executor plugin. metrics=[files_created=4, gcs_api_server_not_implemented_error_count=0, gcs_api_server_timeout_count=0, action_http_post_request_failures=0, op_get_list_status_result_size=2, op_open=0, gcs_api_client_unauthorized_response_count=0, action_http_head_request_failures=0, stream_read_close_operations=0, stream_read_bytes_backwards_on_seek=0, exception_count=91, gcs_api_total_request_count=128, op_create=4, gcs_api_client_bad_request_count=0, op_create_non_recursive=0, gcs_api_client_gone_response_count=0, stream_write_operations=6552, stream_read_operations=0, gcs_api_client_request_timeout_count=0, op_rename=2, op_get_file_status=15, stream_read_total_bytes=0, op_glob_status=2, stream_read_exceptions=0, action_http_get_request_failures=0, op_exists=0, stream_write_bytes=156334, op_xattr_list=0, stream_write_exceptions=0, gcs_api_server_unavailable_count=0, directories_created=0, files_delete_rejected=0, op_xattr_get_named=0, op_hsync=0, stream_read_operations_incomplete=0, op_delete=5, stream_read_bytes=0, gcs_api_client_non_found_response_count=56, gcs_api_client_requested_range_not_statisfiable_count=0, op_hflush=0, op_list_status=2, op_xattr_get_named_map=0, gcs_api_client_side_error_count=146, op_get_file_checksum=0, action_http_delete_request_failures=0, gcs_api_server_internal_error_count=0, stream_read_seek_bytes_skipped=0, stream_write_close_operations=4, op_list_files=0, files_deleted=0, op_mkdirs=2, gcs_api_client_rate_limit_error_count=0, action_http_put_request_failures=0, gcs_api_server_bad_gateway_count=0, stream_read_seek_backward_operations=0, gcs_api_server_side_error_count=0, action_http_patch_request_failures=0, stream_read_seek_operations=0, stream_read_seek_forward_operations=0, gcs_api_client_precondition_failed_response_count=2, directories_deleted=0, op_xattr_get_map=0, delegation_tokens_issued=0, op_create_min=84, op_delete_min=43, op_mkdirs_min=147, op_create_non_recursive_min=0, op_glob_status_min=55, op_hsync_min=0, op_xattr_get_named_min=0, op_list_status_min=26, op_xattr_get_named_map_min=0, stream_read_close_operations_min=0, stream_read_operations_min=0, stream_read_seek_operations_min=0, op_hflush_min=0, op_xattr_get_map_min=0, op_xattr_list_min=0, stream_write_operations_min=0, op_get_file_status_min=23, op_open_min=0, op_rename_min=269, delegation_tokens_issued_min=0, stream_write_close_operations_min=90, stream_read_close_operations_max=0, stream_read_operations_max=0, stream_read_seek_operations_max=0, op_hflush_max=0, op_xattr_list_max=0, op_xattr_get_map_max=0, op_xattr_get_named_max=0, op_create_non_recursive_max=0, op_glob_status_max=378, op_get_file_status_max=332, stream_write_close_operations_max=171, op_open_max=0, delegation_tokens_issued_max=0, op_mkdirs_max=262, op_rename_max=289, op_create_max=147, op_delete_max=257, op_list_status_max=28, op_xattr_get_named_map_max=0, stream_write_operations_max=3, op_hsync_max=0, op_list_status_mean=27, stream_read_close_operations_mean=0, op_open_mean=0, op_xattr_get_named_map_mean=0, op_xattr_list_mean=0, op_mkdirs_mean=204, stream_write_close_operations_mean=125, op_rename_mean=279, op_hsync_mean=0, delegation_tokens_issued_mean=0, stream_read_operations_mean=0, op_xattr_get_map_mean=0, op_create_mean=119, op_glob_status_mean=216, op_delete_mean=165, stream_read_seek_operations_mean=0, stream_write_operations_mean=0, op_create_non_recursive_mean=0, op_hflush_mean=0, op_xattr_get_named_mean=0, op_get_file_status_mean=57, stream_write_operations_duration=0, stream_read_operations_duration=0]\n",
            "2024-11-05 03:29:44,936 - INFO - The project execution is successful! ðŸŽ‰\n",
            "Job [f0b9ce7f9ed14764b564bbc847cd1d93] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/driveroutput\n",
            "jobUuid: 78b24d8a-284a-3577-b0df-397f5e98ca73\n",
            "placement:\n",
            "  clusterName: cluster-for-eq-project\n",
            "  clusterUuid: 7add3a5b-0925-4969-9de0-3d7a3e8b6b8a\n",
            "pysparkJob:\n",
            "  fileUris:\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/staging/my_spark_job_historical.zip\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/staging/default_compute_key.json\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/staging/main_daily.py\n",
            "  pythonFileUris:\n",
            "  - gs://dataproc-staging-us-central1-771574848243-4hclxhic/google-cloud-dataproc-metainfo/7add3a5b-0925-4969-9de0-3d7a3e8b6b8a/jobs/f0b9ce7f9ed14764b564bbc847cd1d93/staging/my_spark_job_historical.zip\n",
            "reference:\n",
            "  jobId: f0b9ce7f9ed14764b564bbc847cd1d93\n",
            "  projectId: project-dipakraj1\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2024-11-05T03:29:45.491672Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2024-11-05T03:29:16.140731Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2024-11-05T03:29:16.163749Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2024-11-05T03:29:16.388062Z'\n"
          ]
        }
      ],
      "source": [
        "# run main_daily.py\n",
        "\n",
        "!gcloud dataproc jobs submit pyspark \\\n",
        "    --cluster cluster-for-eq-project \\\n",
        "    --region us-central1 \\\n",
        "    --files my_spark_job_historical.zip,default_compute_key.json \\\n",
        "    --py-files my_spark_job_historical.zip \\\n",
        "    main_daily.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}